{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torchvision.models\n",
    "from PIL import Image\n",
    "import numpy as nps\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.autoencoder import FeatureEncoder\n",
    "from lib.attention_model import AttentionLayer\n",
    "from lib.correspondence_datamodule import CorrespondenceDataModule\n",
    "from lib.warping import *\n",
    "from externals.d2net.lib.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megadepth_path = \"/mnt/c/Users/phill/polybox/Deep Learning/MegaDepthDataset\"\n",
    "data_module = CorrespondenceDataModule(base_path=megadepth_path, batch_size=1)\n",
    "data_module.prepare_data()\n",
    "data_module.setup(stage='fit')\n",
    "dl_train = data_module.train_dataloader()\n",
    "dl_val = data_module.val_dataloader()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_margin_loss(x1_encoded, x2_encoded, attentions1, \\\n",
    "    attentions2, correspondences, idx, margin=1, safe_radius=4, scaling_steps=3, plot=False):\n",
    "    \"\"\"\n",
    "    x1_encoded: 1xBx48xWxH (dense_features1) (usually x32x32)\n",
    "    x2_encoded: 1xBx48xWxH (dense_features2) (usually x32x32)\n",
    "    attentions1: 1xBx1xWxH (scores1)\n",
    "    attentions2: 1xBx1xWxH (scores2)\n",
    "    correspondences (image1: 1xBx3x256x256 = 1xBx3xw1xh1, ...) -> 32 * 8 = 256 --> 3 scaling steps\n",
    "    idx: batch index (basically loop over all images in the batch)\n",
    "    out: scalar tensor (1)\n",
    "    \"\"\"\n",
    "    loss = torch.tensor(np.array([0], dtype=np.float32), device=device)\n",
    "    \n",
    "    depth1 = correspondences['depth1'][idx] # [h1, w1]???\n",
    "    intrinsics1 = correspondences['intrinsics1'][idx]  # [3, 3]\n",
    "    pose1 = correspondences['pose1'][idx].view(4, 4)  # [4, 4]\n",
    "    bbox1 = correspondences['bbox1'][idx]  # [2]\n",
    "\n",
    "    depth2 = correspondences['depth2'][idx]\n",
    "    intrinsics2 = correspondences['intrinsics2'][idx]\n",
    "    pose2 = correspondences['pose2'][idx].view(4, 4)\n",
    "    bbox2 = correspondences['bbox2'][idx]\n",
    "    \n",
    "\n",
    "    # Network output\n",
    "    dense_features1 = x1_encoded[idx] #48x32x32\n",
    "    c, h1, w1 = dense_features1.size()\n",
    "    scores1 = attentions1[idx].view(-1) #1x1024 (ids format)\n",
    "\n",
    "    dense_features2 = x2_encoded[idx]\n",
    "    _, h2, w2 = dense_features2.size()\n",
    "    scores2 = attentions2[idx].view(-1)\n",
    "\n",
    "    all_descriptors1 = F.normalize(dense_features1.view(c, -1), dim=0)# 48x1024, row-major\n",
    "    descriptors1 = all_descriptors1\n",
    "\n",
    "    all_descriptors2 = F.normalize(dense_features2.view(c, -1), dim=0)# 48x1024\n",
    "\n",
    "    # Warp the positions from image 1 to image 2\n",
    "    fmap_pos1 = grid_positions(h1, w1, device) #feature positions, 2x(32*32)=2x1024 [y,x]-format -> [[0,0],[0,1], ...[32,32]]\n",
    "    pos1 = upscale_positions(fmap_pos1, scaling_steps=scaling_steps) # feature positions in 256x256, [y,x]-format -> [[0,0],[0,11.5], ...[256,256]]\n",
    "    #ids: matching ids in sequence (256*256)\n",
    "    #default pos1 has ids [0, ..., 1024]\n",
    "    # now ids says which of these are valid based on relative transformation between them, e.g.\n",
    "    # [5, 28, 32, ...,1020]\n",
    "    # so a legal correspondence would be pos1[:,5]<-->pos2[:,5]\n",
    "    try:\n",
    "        pos1, pos2, ids = warp(pos1,\n",
    "            depth1, intrinsics1, pose1, bbox1,\n",
    "            depth2, intrinsics2, pose2, bbox2)\n",
    "    except EmptyTensorError:\n",
    "        return loss\n",
    "    fmap_pos1 = fmap_pos1[:, ids] #uv-positions on 32x32 grid, but in list format 2xlen(ids)\n",
    "    descriptors1 = descriptors1[:, ids] #again as list 48xlen(ids)\n",
    "    scores1 = scores1[ids] #again as list 1xlen(ids)\n",
    "\n",
    "    # Skip the pair if not enough GT correspondences are available\n",
    "    if ids.size(0) < 128:\n",
    "        return loss\n",
    "\n",
    "    # Descriptors at the corresponding positions\n",
    "    fmap_pos2 = torch.round(\n",
    "        downscale_positions(pos2, scaling_steps=scaling_steps)\n",
    "    ).long()\n",
    "    descriptors2 = F.normalize(\n",
    "        dense_features2[:, fmap_pos2[0, :], fmap_pos2[1, :]],\n",
    "        dim=0\n",
    "    )\n",
    "    positive_distance = 2 - 2 * (\n",
    "        descriptors1.t().unsqueeze(1) @ descriptors2.t().unsqueeze(2)\n",
    "    ).squeeze() #p(c) in paper, ||dA -dB||\n",
    "\n",
    "    all_fmap_pos2 = grid_positions(h2, w2, device)\n",
    "    position_distance = torch.max(\n",
    "        torch.abs(\n",
    "            fmap_pos2.unsqueeze(2).float() -\n",
    "            all_fmap_pos2.unsqueeze(1)\n",
    "        ),\n",
    "        dim=0\n",
    "    )[0] # all other distances within image2 (distance is feature-metric norm!)\n",
    "    is_out_of_safe_radius = position_distance > safe_radius\n",
    "    distance_matrix = 2 - 2 * (descriptors1.t() @ all_descriptors2) # ||dA -dN2||\n",
    "    negative_distance2 = torch.min(\n",
    "        distance_matrix + (1 - is_out_of_safe_radius.float()) * 10., #weird\n",
    "        dim=1\n",
    "    )[0] \n",
    "\n",
    "    all_fmap_pos1 = grid_positions(h1, w1, device)\n",
    "    position_distance = torch.max(\n",
    "        torch.abs(\n",
    "            fmap_pos1.unsqueeze(2).float() -\n",
    "            all_fmap_pos1.unsqueeze(1)\n",
    "        ),\n",
    "        dim=0\n",
    "    )[0]\n",
    "    is_out_of_safe_radius = position_distance > safe_radius\n",
    "    distance_matrix = 2 - 2 * (descriptors2.t() @ all_descriptors1)  # ||dB -dN1||\n",
    "    negative_distance1 = torch.min(\n",
    "        distance_matrix + (1 - is_out_of_safe_radius.float()) * 10., #weird\n",
    "        dim=1\n",
    "    )[0]\n",
    "\n",
    "    diff = positive_distance - torch.min(\n",
    "        negative_distance1, negative_distance2\n",
    "    ) # (n(c))\n",
    "\n",
    "    scores2 = scores2[fmap_pos2[0, :], fmap_pos2[1, :]]\n",
    "\n",
    "    loss = loss + (\n",
    "        torch.sum(scores1 * scores2 * F.relu(margin + diff)) /\n",
    "        torch.sum(scores1 * scores2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of lib.loss failed: Traceback (most recent call last):\n",
      "  File \"/home/phil/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/phil/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/phil/Projects/deep_learning/lib/loss.py\", line 8, in <module>\n",
      "    from externals.d2net.utils import *\n",
      "ModuleNotFoundError: No module named 'externals.d2net.utils'\n",
      "]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TripletLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-f2ce52dac42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeature_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mattention_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx1_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx2_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/deep_learning/lib/attention_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, feature_encoder)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTripletLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TripletLoss' is not defined"
     ]
    }
   ],
   "source": [
    "x1 = batch['image1']\n",
    "x2 = batch[\"image2\"]\n",
    "feature_encoder = FeatureEncoder()\n",
    "attention_model = AttentionLayer(feature_encoder)\n",
    "x1_encoded = attention_model.concat_layers(attention_model.feature_encoder.forward(x1))\n",
    "x2_encoded = attention_model.concat_layers(attention_model.feature_encoder.forward(x2))\n",
    "\n",
    "x1_encoded.requires_grad = False\n",
    "x2_encoded.requires_grad = False\n",
    "# y1 = attention_model.forward(x1_encoded)\n",
    "# y2 = attention_model.forward(x2_encoded)\n",
    "\n",
    "# loss = attention_model.loss(x1_encoded, x2_encoded, y1, y2, correspondence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a new training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.91s/it]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of lib.loss failed: Traceback (most recent call last):\n",
      "  File \"/home/phil/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/phil/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/phil/Projects/deep_learning/lib/loss.py\", line 8, in <module>\n",
      "    from externals.d2net.utils import *\n",
      "ModuleNotFoundError: No module named 'externals.d2net.utils'\n",
      "]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TripletLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-f2ce52dac42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeature_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mattention_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx1_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx2_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/deep_learning/lib/attention_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, feature_encoder)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTripletLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TripletLoss' is not defined"
     ]
    }
   ],
   "source": [
    "x1 = batch['image1']\n",
    "x2 = batch[\"image2\"]\n",
    "feature_encoder = FeatureEncoder()\n",
    "attention_model = AttentionLayer(feature_encoder)\n",
    "x1_encoded = attention_model.concat_layers(attention_model.feature_encoder.forward(x1))\n",
    "x2_encoded = attention_model.concat_layers(attention_model.feature_encoder.forward(x2))\n",
    "\n",
    "x1_encoded.requires_grad = False\n",
    "x2_encoded.requires_grad = False\n",
    "# y1 = attention_model.forward(x1_encoded)\n",
    "# y2 = attention_model.forward(x2_encoded)\n",
    "\n",
    "# loss = attention_model.loss(x1_encoded, x2_encoded, y1, y2, correspondence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/c/Users/phill/polybox/Deep Learning/MegaDepthDataset/train_scenes.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6438b1f419ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmegadepth_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/mnt/c/Users/phill/polybox/Deep Learning/MegaDepthDataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorrespondenceDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmegadepth_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdl_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_prepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/deep_learning/lib/correspondence_datamodule.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;34m\"\"\" Builds MegaDepthDataset(s) with extraction \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMegaDepthDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscene_list_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{base_path}/train_scenes.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscene_info_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{base_path}/scene_info\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/deep_learning/lib/megadepth_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, scene_list_path, scene_info_path, base_path, train, preprocessing, min_overlap_ratio, max_overlap_ratio, max_scale_ratio, pairs_per_scene, image_size, transform)\u001b[0m\n\u001b[1;32m     35\u001b[0m     ):\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_list_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/c/Users/phill/polybox/Deep Learning/MegaDepthDataset/train_scenes.txt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit03d3002403754c398b1cb8f8a691cb17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
