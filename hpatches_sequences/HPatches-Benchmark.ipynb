{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "from skimage.feature import match_descriptors\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import ProjectiveTransform\n",
    "import cv2\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = True \n",
    "plot_idx = [80] # which images should be plottet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file removed\n"
     ]
    }
   ],
   "source": [
    "# delete cache of our-model\n",
    "file = 'cache/our-model.npy'\n",
    "if os.path.exists(file):\n",
    "    os.remove(file)\n",
    "    print('file removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new methods here.\n",
    "# methods = ['hesaff', 'hesaffnet', 'delf', 'delf-new', 'superpoint', 'd2-net', 'd2-net-trained']\n",
    "# names = ['Hes. Aff. + Root-SIFT', 'HAN + HN++', 'DELF', 'DELF New', 'SuperPoint', 'D2-Net', 'D2-Net Trained']\n",
    "# colors = ['black', 'orange', 'red', 'red', 'blue', 'purple', 'purple']\n",
    "# linestyles = ['-', '-', '-', '--', '-', '-', '--']\n",
    "methods = ['our-model','hesaff', 'hesaffnet', 'delf', 'delf-new', 'superpoint', 'lf-net', 'd2-net']\n",
    "names = ['our-model','Hes. Aff. + Root-SIFT', 'HAN + HN++', 'DELF', 'DELF New', 'SuperPoint', 'LF-Net', 'D2-Net', 'D2-Net MS', 'D2-Net Trained', 'D2-Net Trained MS']\n",
    "colors = ['pink','black', 'orange', 'red', 'red', 'blue', 'brown', 'purple', 'green', 'purple', 'green']\n",
    "linestyles = ['-','-', '-', '-', '--', '-', '-', '-', '-', '--', '--']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here if you want to use top K or all features.\n",
    "# top_k = 2000\n",
    "top_k = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_i = 52\n",
    "n_v = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '~/Downloads/hpatches-sequences/hpatches-sequences-release'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = [1, 15]\n",
    "rng = np.arange(lim[0], lim[1] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnn_matcher(descriptors_a, descriptors_b):\n",
    "    device = descriptors_a.device\n",
    "    sim = descriptors_a @ descriptors_b.t()\n",
    "    nn12 = torch.max(sim, dim=1)[1]\n",
    "    nn21 = torch.max(sim, dim=0)[1]\n",
    "    ids1 = torch.arange(0, sim.shape[0], device=device)\n",
    "    mask = (ids1 == nn21[nn12])\n",
    "    matches = torch.stack([ids1[mask], nn12[mask]])\n",
    "    return matches.t().data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path, index):\n",
    "    path = os.path.join(path, f\"{index}.ppm\")\n",
    "    image = imageio.imread(os.path.join(dataset_path, path))\n",
    "    if len(image.shape) == 2:\n",
    "        image = image[:, :, np.newaxis]\n",
    "        image = np.repeat(image, 3, -1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def benchmark_features(read_feats):\n",
    "    seq_names = sorted(os.listdir(dataset_path))\n",
    "\n",
    "    n_feats = []\n",
    "    n_matches = []\n",
    "    seq_type = []\n",
    "    i_err = {thr: 0 for thr in rng}\n",
    "    v_err = {thr: 0 for thr in rng}\n",
    "\n",
    "    for seq_idx, seq_name in tqdm(enumerate(seq_names), total=len(seq_names)):\n",
    "        keypoints_a, descriptors_a = read_feats(seq_name, 1)\n",
    "        n_feats.append(keypoints_a.shape[0])\n",
    "\n",
    "##\n",
    "        axs = None \n",
    "        image1_np = None\n",
    "        if visualize and seq_idx in plot_idx:\n",
    "            _, axs = plt.subplots(5,1, figsize=(20, 20))\n",
    "            image1_np = read_image(seq_name, 1)\n",
    "##/\n",
    "        for im_idx in range(2, 7):\n",
    "            keypoints_b, descriptors_b = read_feats(seq_name, im_idx)\n",
    "            n_feats.append(keypoints_b.shape[0])\n",
    "\n",
    "            matches = mnn_matcher(\n",
    "                torch.from_numpy(descriptors_a).to(device=device), \n",
    "                torch.from_numpy(descriptors_b).to(device=device)\n",
    "            )\n",
    "\n",
    "            image2_np = None\n",
    "            if visualize and seq_idx in plot_idx:\n",
    "                image2_np = read_image(seq_name, im_idx)\n",
    "                #matches = match_descriptors(descriptors_a, descriptors_b, cross_check=True)\n",
    "                #matches = mnn_matcher(\n",
    "                #    torch.from_numpy(descriptors_a).to(device=device), \n",
    "                #    torch.from_numpy(descriptors_b).to(device=device)\n",
    "                #)\n",
    "                keypoints_left = keypoints_a[matches[:, 0], : 2]\n",
    "                keypoints_right = keypoints_b[matches[:, 1], : 2]\n",
    "                np.random.seed(0)\n",
    "                model, inliers = ransac(\n",
    "                    (keypoints_left, keypoints_right),\n",
    "                    ProjectiveTransform, min_samples=4,\n",
    "                    residual_threshold=4, max_trials=10000\n",
    "                )\n",
    "                n_inliers = np.sum(inliers)\n",
    "                print('Number of inliers: %d.' % n_inliers)\n",
    "                inlier_keypoints_left = [cv2.KeyPoint(point[0], point[1], 1) for point in keypoints_left[inliers]]\n",
    "                inlier_keypoints_right = [cv2.KeyPoint(point[0], point[1], 1) for point in keypoints_right[inliers]]\n",
    "                placeholder_matches = [cv2.DMatch(idx, idx, 1) for idx in range(n_inliers)]\n",
    "                image3 = cv2.drawMatches(image1_np, inlier_keypoints_left, image2_np, inlier_keypoints_right, placeholder_matches, None)\n",
    "\n",
    "                #print(keypoints_left[inliers])\n",
    "                axs[im_idx-2].imshow(image3)\n",
    "\n",
    "          \n",
    "            homography = np.loadtxt(os.path.join(dataset_path, seq_name, \"H_1_\" + str(im_idx)))\n",
    "            \n",
    "            pos_a = keypoints_a[matches[:, 0], : 2] \n",
    "            pos_a_h = np.concatenate([pos_a, np.ones([matches.shape[0], 1])], axis=1)\n",
    "            pos_b_proj_h = np.transpose(np.dot(homography, np .transpose(pos_a_h)))\n",
    "            pos_b_proj = pos_b_proj_h[:, : 2] / pos_b_proj_h[:, 2 :]\n",
    "\n",
    "            pos_b = keypoints_b[matches[:, 1], : 2]\n",
    "\n",
    "            dist = np.sqrt(np.sum((pos_b - pos_b_proj) ** 2, axis=1))\n",
    "\n",
    "            n_matches.append(matches.shape[0])\n",
    "            seq_type.append(seq_name[0])\n",
    "            \n",
    "            if dist.shape[0] == 0:\n",
    "                dist = np.array([float(\"inf\")])\n",
    "            \n",
    "            for thr in rng:\n",
    "                if seq_name[0] == 'i':\n",
    "                    i_err[thr] += np.mean(dist <= thr)\n",
    "                else:\n",
    "                    v_err[thr] += np.mean(dist <= thr)\n",
    "        \n",
    "        if visualize and seq_idx in plot_idx:\n",
    "            print(seq_name)\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "    seq_type = np.array(seq_type)\n",
    "    n_feats = np.array(n_feats)\n",
    "    n_matches = np.array(n_matches)\n",
    "    \n",
    "    return i_err, v_err, [seq_type, n_feats, n_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(stats):\n",
    "    seq_type, n_feats, n_matches = stats\n",
    "    print('# Features: {:f} - [{:d}, {:d}]'.format(np.mean(n_feats), np.min(n_feats), np.max(n_feats)))\n",
    "    print('# Matches: Overall {:f}, Illumination {:f}, Viewpoint {:f}'.format(\n",
    "        np.sum(n_matches) / ((n_i + n_v) * 5), \n",
    "        np.sum(n_matches[seq_type == 'i']) / (n_i * 5), \n",
    "        np.sum(n_matches[seq_type == 'v']) / (n_v * 5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_read_function(method, extension='ppm'):\n",
    "    def read_function(seq_name, im_idx):\n",
    "        aux = np.load(os.path.join(dataset_path, seq_name, '%d.%s.%s' % (im_idx, extension, method)))\n",
    "        if top_k is None:\n",
    "            return aux['keypoints'], aux['descriptors']\n",
    "        else:\n",
    "            assert('scores' in aux)\n",
    "            ids = np.argsort(aux['scores'])[-top_k :]\n",
    "            return aux['keypoints'][ids, :], aux['descriptors'][ids, :]\n",
    "    return read_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift_to_rootsift(descriptors):\n",
    "    return np.sqrt(descriptors / np.expand_dims(np.sum(np.abs(descriptors), axis=1), axis=1) + 1e-16)\n",
    "def parse_mat(mat):\n",
    "    keypoints = mat['keypoints'][:, : 2]\n",
    "    raw_descriptors = mat['descriptors']\n",
    "    l2_norm_descriptors = raw_descriptors / np.expand_dims(np.sum(raw_descriptors ** 2, axis=1), axis=1)\n",
    "    descriptors = sift_to_rootsift(l2_norm_descriptors)\n",
    "    if top_k is None:\n",
    "        return keypoints, descriptors\n",
    "    else:\n",
    "        assert('scores' in mat)\n",
    "        ids = np.argsort(mat['scores'][0])[-top_k :]\n",
    "        return keypoints[ids, :], descriptors[ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if top_k is None:\n",
    "    cache_dir = 'cache'\n",
    "else:\n",
    "    cache_dir = 'cache-top'\n",
    "if not os.path.isdir(cache_dir):\n",
    "    os.mkdir(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our-model\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/Downloads/hpatches-sequences/hpatches-sequences-release'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ea792f63b18f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmark_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-edb0138f39db>\u001b[0m in \u001b[0;36mbenchmark_features\u001b[0;34m(read_feats)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbenchmark_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mseq_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/Downloads/hpatches-sequences/hpatches-sequences-release'"
     ]
    }
   ],
   "source": [
    "for method in methods:\n",
    "    output_file = os.path.join(cache_dir, method + '.npy')\n",
    "    print(method)\n",
    "    if method == 'hesaff':\n",
    "        read_function = lambda seq_name, im_idx: parse_mat(loadmat(os.path.join(dataset_path, seq_name, '%d.ppm.hesaff' % im_idx), appendmat=False))\n",
    "    else:\n",
    "        if method == 'delf' or method == 'delf-new':\n",
    "            read_function = generate_read_function(method, extension='png')\n",
    "        else:\n",
    "            read_function = generate_read_function(method)\n",
    "    if os.path.exists(output_file):\n",
    "        print('Loading precomputed errors...')\n",
    "        errors[method] = np.load(output_file, allow_pickle=True)\n",
    "    else:\n",
    "        errors[method] = benchmark_features(read_function)\n",
    "        np.save(output_file, errors[method])\n",
    "    summary(errors[method][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_lim = [1, 10]\n",
    "plt_rng = np.arange(plt_lim[0], plt_lim[1] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('axes', titlesize=25)\n",
    "plt.rc('axes', labelsize=25)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "for method, name, color, ls in zip(methods, names, colors, linestyles):\n",
    "    i_err, v_err, _ = errors[method]\n",
    "    plt.plot(plt_rng, [(i_err[thr] + v_err[thr]) / ((n_i + n_v) * 5) for thr in plt_rng], color=color, ls=ls, linewidth=3, label=name)\n",
    "plt.title('Overall')\n",
    "plt.xlim(plt_lim)\n",
    "plt.xticks(plt_rng)\n",
    "plt.ylabel('MMA')\n",
    "plt.ylim([0, 1])\n",
    "plt.grid()\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "for method, name, color, ls in zip(methods, names, colors, linestyles):\n",
    "    i_err, v_err, _ = errors[method]\n",
    "    plt.plot(plt_rng, [i_err[thr] / (n_i * 5) for thr in plt_rng], color=color, ls=ls, linewidth=3, label=name)\n",
    "plt.title('Illumination')\n",
    "plt.xlabel('threshold [px]')\n",
    "plt.xlim(plt_lim)\n",
    "plt.xticks(plt_rng)\n",
    "plt.ylim([0, 1])\n",
    "plt.gca().axes.set_yticklabels([])\n",
    "plt.grid()\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "for method, name, color, ls in zip(methods, names, colors, linestyles):\n",
    "    i_err, v_err, _ = errors[method]\n",
    "    plt.plot(plt_rng, [v_err[thr] / (n_v * 5) for thr in plt_rng], color=color, ls=ls, linewidth=3, label=name)\n",
    "plt.title('Viewpoint')\n",
    "plt.xlim(plt_lim)\n",
    "plt.xticks(plt_rng)\n",
    "plt.ylim([0, 1])\n",
    "plt.gca().axes.set_yticklabels([])\n",
    "plt.grid()\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "if top_k is None:\n",
    "    plt.savefig('hseq.pdf', bbox_inches='tight', dpi=300)\n",
    "else:\n",
    "    plt.savefig('hseq-top.pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
